---
title: "Week 11a Lecture"
author: "Mac"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Load Libraries

```{r}
library(here)
library(tidyverse)
library(tidytext)
library(wordcloud2)
library(janeaustenr)
```

# Data Analysis

A string and a character are the same thing. Both have quotation marks.

```{r}
words<-"This is a string"
words
```

You can have several strings in a vector

```{r}
words_vector<-c("Apples", "Bananas","Oranges")
words_vector
```

# Manipulation

paste words together

```{r}
paste("High temp", "Low pH")
```

add a dash in between words

```{r}
paste("High temp", "Low pH", sep = "-")
```

remove the spaces between words

```{r}
paste0("High temp", "Low pH")
```

working with vectors

```{r}
shapes <- c("Square", "Circle", "Triangle")
paste("My favorite shape is a", shapes)
```

```{r}
two_cities <- c("best", "worst")
paste("It was the", two_cities, "of times.")
```

## Manipulation: individual characters

```{r}
shapes # vector of shapes
```

```{r}
str_length(shapes) # how many letters are in each word?
```

how to extract specific characters

```{r}
seq_data<-c("ATCCCGTC")
str_sub(seq_data, start = 2, end = 4) # extract the 2nd to 4th AA
```

modify strings

```{r}
str_sub(seq_data, start = 3, end = 3) <- "A" # add an A in the 3rd position
seq_data
```

duplicate patterns in strings

```{r}
str_dup(seq_data, times = c(2, 3)) # times is the number of times to duplicate each string
```

# Whitespace

```{r}
badtreatments<-c("High", " High", "High ", "Low", "Low")
badtreatments
```

remove white space

```{r}
str_trim(badtreatments) # this removes both
```

just remove white space from one side or the other

```{r}
str_trim(badtreatments, side = "left") # this removes left
```

add white space to either side

```{r}
str_pad(badtreatments, 5, side = "right") # add a white space to the right side after the 5th character
```

add a character instead of white space

```{r}
str_pad(badtreatments, 5, side = "right", pad = "1") # add a 1 to the right side after the 5th character
```

# Locale sensitive

make everything uppercase

```{r}
x<-"I love R!"
str_to_upper(x)
```

make it lower case

```{r}
str_to_lower(x)
```

make it title case (capitalize first letter of every word)

```{r}
str_to_title(x)
```

# Pattern matching

view a specific pattern in a vector of strings

```{r}
data<-c("AAA", "TATA", "CTAG", "GCTT")
```

```{r}
# find all the strings with an A
str_view(data, pattern = "A")
```

detect a specific pattern

```{r}
str_detect(data, pattern = "A")
```

```{r}
str_detect(data, pattern = "AT")
```

locate a pattern

```{r}
str_locate(data, pattern = "AT")
```

# Metacharacters

have meaning in strings beyond what they represent in human language

```{r}
vals<-c("a.b", "b.c", "c.d")
```

replace all the "." with a space

```{r}
#string, pattern, replace
str_replace(vals, "\\.", " ")
```

# Aside about the functions

str_replace() only replaces the first instance

```{r}
vals<-c("a.b.c", "b.c.d", "c.d.e")
#string, pattern, replace
str_replace(vals, "\\.", " ")
```

str_replace_all() replaces all instances

```{r}
#string, pattern, replace
str_replace_all(vals, "\\.", " ")
```

# Sequences

subset vector to only keep strings with digits

```{r}
val2<-c("test 123", "test 456", "test")
str_subset(val2, "\\d")
```

# Character class

count the number of lowercase vowels in each string

```{r}
str_count(val2, "[aeiou]")
```

```{r}
# count any digit
str_count(val2, "[0-9]")
```

# Example: find the phone numbers

```{r}
strings<-c("550-153-7578",
         "banana",
         "435.114.7586",
         "home: 672-442-6739")
```

make a regex that finds all the strings that contain a phone number

```{r}
phone <- "([2-9][0-9]{2})[- .]([0-9]{3})[- .]([0-9]{4})"
```

my alternate, less efficient version to show how it works

```{r}
phone2 <- "([2-9][0-9][09])[- .]([0-9][0-9][0-9])[- .]([0-9][0-9][0-9][0-9])"
```

```{r}
# Which strings contain phone numbers?
str_detect(strings, phone)
```

```{r}
# subset only the strings with phone numbers
test<-str_subset(strings, phone)
test
```

# Think, pair, share

```{r}
test %>%
  str_replace_all(pattern = "\\.", replacement = "-") %>% # replace periods with -
  str_replace_all(pattern = "[a-zA-Z]|\\:", replacement = "") %>% # remove all the things we don't want
  str_trim() # trim the white space
```

my version that uses this for nucleotide sequences

```{r}
seq <- " ATG-CAT-GCA-TGC "

seq_RNA <- seq %>%
  str_replace_all(pattern = "T", replacement = "U") %>%
  str_replace_all(pattern = "-", replacement = "") %>%
  str_trim()

seq
seq_RNA
```

```{r}
seq <- " ATG-CAT-GCA-TGC "

seq_comp <- seq %>%
  str_replace_all(pattern = c("A", "T", "G", "C"), replacement = c("T", "A", "C", "G")) %>%
  str_replace_all(pattern = "-", replacement = "") %>%
  str_trim()

seq
seq_comp
```

# tidytext

austen_books(): function to get all of the text from all of Jane Austen's books
- Do NOT use View(), this function has the entire text of all of here books! You'll crash your computer.

```{r}
# explore it
head(austen_books())
```

```{r}
tail(austen_books())
```

clean it up, add a column for line and chapter

```{r}
original_books <- austen_books() %>% # get all of Jane Austen's books
  group_by(book) %>%
  mutate(line = row_number(), # find every line
         chapter = cumsum(str_detect(text, regex("^chapter [\\divxlc]", # count the chapters (starts with the word chapter followed by a digit or roman numeral)
                                                 ignore_case = TRUE)))) %>% #ignore lower or uppercase
  ungroup() # ungroup it so we have a dataframe again
# don't try to view the entire thing... its >73000 lines...
head(original_books)
```

```{r}
tidy_books <- original_books %>%
  unnest_tokens(output = word, input = text) # add a column named word, with the input as the text column
head(tidy_books) # there are now >725,000 rows. Don't view the entire thing!
```

```{r}
#see an example of all the stopwords
head(get_stopwords())
```

```{r}
cleaned_books <- tidy_books %>%
  anti_join(get_stopwords()) # dataframe without the stopwords
```

```{r}
head(cleaned_books)
```

```{r}
cleaned_books %>%
  count(word, sort = TRUE)
```

```{r}
sent_word_counts <- tidy_books %>%
  inner_join(get_sentiments()) %>% # only keep pos or negative words
  count(word, sentiment, sort = TRUE) # count them
```

```{r}
sent_word_counts %>%
  filter(n > 150) %>% # take only if there are over 150 instances of it
  mutate(n = ifelse(sentiment == "negative", -n, n)) %>% # add a column where if the word is negative make the count negative
  mutate(word = reorder(word, n)) %>% # sort it so it gows from largest to smallest
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col() +
  coord_flip() +
  labs(y = "Contribution to sentiment")
```

```{r}
words<-cleaned_books %>%
  count(word) %>% # count all the words
  arrange(desc(n))%>% # sort the words
  slice(1:100) #take the top 100
wordcloud2(words, shape = 'triangle', size=0.3) # make a wordcloud out of the top 100 words
```

